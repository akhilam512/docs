---
title: Overview
description: 'Pipecat is a framework for building voice-enabled, real-time, multimodal AI applications'
---

Pipecat is an open source Python framework that handles the complex orchestration of AI services, network transport, audio processing, and multimodal interactions. "Multimodal" means you can use any combination of audio, video, images, and/or text in your interactions. And "real-time" means that things are happening quickly enough that it feels conversationalâ€”a "back-and-forth" with a bot, not submitting a query and waiting for results.

## What You Can Build

- **Voice Assistants**: [Natural, real-time conversations with AI](https://demo.dailybots.ai/)
- **Interactive Agents**: Personal coaches and meeting assistants
- **Multimodal Apps**: Combine voice, video, images, and text
- **Creative Tools**: [Story-telling experiences](https://storytelling-chatbot.fly.dev/) and social companions
- **Business Solutions**: [Customer intake flows](https://www.youtube.com/watch?v=lDevgsp9vn0) and support bots
- **Complex conversational flows**: [Refer to Pipecat Flows](https://github.com/pipecat-ai/pipecat-flows) to learn more

## How It Works

The flow of interactions in a Pipecat application is typically straightforward:

1. The bot says something
2. The user says something
3. The bot says something
4. The user says something

This continues until the conversation naturally ends. While this flow seems simple, making it feel natural requires sophisticated real-time processing.

### Real-time Processing

Consider a voice-based interaction where the bot needs to respond:

1. Receive the user's audio input
2. Transcribe the user's speech as they're talking
3. Process the transcription through an LLM
4. Convert the response to speech
5. Play the audio to the user

For multimodal models (like GPT-4V), the flow might be different:

1. Receive the user's audio, video, and image inputs
2. Process multiple input streams (audio, video, images)
3. Send combined input to the multimodal model
4. Handle various output types (text, generated images, etc.)
5. Coordinate the presentation of multiple outputs

In both cases, Pipecat:

- Processes responses as they stream in
- Handles multiple input/output modalities concurrently
- Manages resource allocation and synchronization
- Coordinates parallel processing tasks

This architecture creates fluid, natural interactions without noticeable delays, whether you're building a simple voice assistant or a complex multimodal application. Pipecat's pipeline architecture is particularly valuable for managing the complexity of real-time, multimodal interactions, ensuring smooth data flow and proper synchronization regardless of the input/output types involved.

## Next Steps

Ready to build your first Pipecat application?

1. Start with [Installation & Setup](/getting-started/installation) to prepare your environment
2. Follow the [Quickstart](/getting-started/quickstart) to run your first example
3. Learn about [Core Concepts](/getting-started/core-concepts) to understand how Pipecat works
4. Explore [Use Cases](/getting-started/use-cases) for implementation examples

## Join Our Community

Need help or want to share your project? Join our [Discord community](https://discord.gg/pipecat) where you can connect with other developers and get support from the Pipecat team.
