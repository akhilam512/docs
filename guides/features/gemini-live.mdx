---
title: "Building with Gemini Multimodal Live"
description: "Create a real-time AI chatbot using Gemini Multimodal Live and Pipecat"
---

This guide will walk you through building a real-time AI chatbot using Gemini Multimodal Live and Pipecat. We'll create a complete application with a Pipecat server and a Pipecat React client that enables natural conversations with an AI assistant.

<Frame>
  ![Pipecat + Gemini Multimodal Live](/images/gemini-client-final.png)
</Frame>

## Key Concepts

Before we dive into implementation, let's cover some important concepts that will help you understand how Pipecat and Gemini work together.

### The Pipeline

At the heart of Pipecat is the pipeline system. A pipeline processes frames of data (audio, video, text) through a series of processors. For our chatbot, the pipeline will:

- Handle audio/video input and output
- Process speech through Gemini
- Manage conversation context
- Control the bot's animated avatar

### Frames and Processors

<AccordionGroup>
  <Accordion title="Frame Types">
    Frames are discrete units of data flowing through the pipeline. Key frame types include:
    - `InputAudioRawFrame` - Raw audio from the user
    - `TTSAudioRawFrame` - Generated speech audio
    - `TranscriptionFrame` - Text transcriptions
    - `TextFrame` - Text responses
  </Accordion>

  <Accordion title="Processors">
    Processors handle specific tasks in the pipeline:
    - `RTVISpeakingProcessor` - Manages speaking states
    - `RTVIUserTranscriptionProcessor` - Handles user transcription
    - `RTVIBotTranscriptionProcessor` - Handles bot transcription
    - `TalkingAnimation` - Controls bot animation
  </Accordion>
</AccordionGroup>

### Gemini Integration

The `GeminiMultimodalLiveLLMService` provides:

- Real-time speech-to-speech conversation
- Built-in transcription
- Context management
- Voice activity detection

<Note>
  Gemini Multimodal Live is designed for server-to-server communication. Our
  server will handle all Gemini interactions, while the client focuses on media
  streaming and UI.
</Note>

## Prerequisites

Before we begin, you'll need:

<CheckboxList>
  - Python 3.10 or higher - Node.js 16 or higher - A Daily API key - A Google
  API key with Gemini access
</CheckboxList>

## Server Implementation

Let's start by setting up the server components. Our server will handle bot management, room creation, and client connections.

### Environment Setup

First, create a new directory and set up your environment:

```bash
mkdir chatbot-demo
cd chatbot-demo
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install pipecat-ai[google]
```

Create a `.env` file:

```ini
DAILY_API_KEY=           # Your Daily API key
GEMINI_API_KEY=          # Your Google API key
DAILY_API_URL=           # Optional: Daily API URL
BOT_IMPLEMENTATION=gemini # Use Gemini implementation
```

### Server Setup (server.py)

Our FastAPI server manages bot instances and handles client connections:

```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pipecat.transports.services.helpers.daily_rest import DailyRESTHelper

app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Daily helper
daily_helpers = {}
bot_procs = {}
```

<Note>
  The server provides two main endpoints: - `/` for Daily Prebuilt access -
  `/connect` for RTVI client connections
</Note>

### Bot Implementation (bot-gemini.py)

The bot implementation is where Gemini and Pipecat come together. Let's break it down:

#### 1. Initialize Gemini Service

```python
llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GEMINI_API_KEY"),
    voice_id="Puck",  # Choose from: Aoede, Charon, Fenrir, Kore, Puck
    transcribe_user_audio=True,
    transcribe_model_audio=True,
)
```

#### 2. Configure Pipeline

```python
pipeline = Pipeline([
    transport.input(),
    context_aggregator.user(),
    llm,
    rtvi_speaking,
    rtvi_user_transcription,
    rtvi_bot_transcription,
    ta,
    rtvi_metrics,
    transport.output(),
    context_aggregator.assistant(),
])
```

<Tip>
  The pipeline order matters! Each processor receives frames from the previous
  processor and passes them to the next.
</Tip>

#### 3. Animation Handling

```python
class TalkingAnimation(FrameProcessor):
    """Manages the bot's visual animation states."""

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        if isinstance(frame, BotStartedSpeakingFrame):
            await self.push_frame(talking_frame)
        elif isinstance(frame, BotStoppedSpeakingFrame):
            await self.push_frame(quiet_frame)
```

## Client Implementation

Our React client uses the RTVI SDK to communicate with the bot. Here's how it works:

### RTVI Provider Setup

```typescript
const transport = new DailyTransport();

const client = new RTVIClient({
  transport,
  params: {
    baseUrl: "http://localhost:7860",
    endpoints: {
      connect: "/connect",
    },
  },
  enableMic: true,
  enableCam: false,
});
```

### Core Components

The main App component structure:

```tsx
function App() {
  return (
    <RTVIProvider>
      <div className="app">
        <StatusDisplay />
        <ConnectButton />
        <BotVideo />
        <DebugDisplay />
        <RTVIClientAudio />
      </div>
    </RTVIProvider>
  );
}
```

<Note>
  `RTVIClientAudio` handles audio input/output automatically. No manual media
  stream management needed!
</Note>

## Running the Application

### 1. Start the Server

```bash
cd server
python server.py
```

### 2. Start the Client

```bash
cd client
npm install
npm start
```

### 3. Testing the Connection

1. Open `http://localhost:3000` in your browser
2. Click "Connect" to join a room
3. Allow microphone access when prompted
4. Start talking with your AI assistant!

## Advanced Topics

### Error Handling

Handle common issues gracefully:

```python
@transport.event_handler("on_error")
async def handle_error(transport, error):
    logger.error(f"Transport error: {error}")
    await task.queue_frame(ErrorFrame(error=str(error)))
```

### Context Management

Maintain conversation context:

```python
context = OpenAILLMContext([
    {
        "role": "system",
        "content": "You are a helpful assistant...",
    }
])
context_aggregator = llm.create_context_aggregator(context)
```

### Voice Activity Detection

Fine-tune VAD settings:

```python
vad_analyzer=SileroVADAnalyzer(
    params=VADParams(
        stop_secs=0.5,
        speech_pad_ms=400,
    )
)
```

## Best Practices

<CardGroup cols={2}>
  <Card title="Audio Configuration" icon="waveform">
    - Use 16kHz input sample rate - Use 24kHz output sample rate - Enable VAD
    for natural conversation
  </Card>

{" "}

<Card title="Error Handling" icon="triangle-exclamation">
  - Handle connection errors - Monitor transcription status - Implement
  reconnection logic
</Card>

{" "}

<Card title="Performance" icon="gauge-high">
  - Enable metrics monitoring - Optimize VAD settings - Monitor token usage
</Card>

  <Card title="User Experience" icon="user">
    - Provide clear status indicators - Show transcription feedback - Handle
    interruptions gracefully
  </Card>
</CardGroup>

## Next Steps

Now that you have a working chatbot, consider these enhancements:

- Add custom avatar animations
- Implement function calling for external integrations
- Add support for multiple languages
- Enhance error recovery and reconnection logic

### Examples

- [Foundational Example](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/26a-gemini-multimodal-live-transcription.py)
  Basic implementation showing core features

- [Simple Chatbot](https://github.com/pipecat-ai/pipecat/tree/main/examples/simple-chatbot)
  Complete client/server implementation

### Learn More

- [Gemini Multimodal Live API Reference](/api-reference/gemini-multimodal-live)
- [Pipecat Pipeline Guide](/guides/concepts/pipeline)
- [RTVI React SDK Documentation](https://docs.pipecat.ai/client/reference/react/introduction)
